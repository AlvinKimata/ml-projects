{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from models.TMC import ETMC, ce_loss\n",
    "import torchvision.transforms as transforms\n",
    "from data.dfdt_dataset import FakeAVCelebDataset\n",
    "from utils.utils import *\n",
    "from utils.logger import create_logger\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_args(parser):\n",
    "    parser.add_argument(\"--batch_sz\", type=int, default=32)\n",
    "    parser.add_argument(\"--train_data_path\", type=str, default=\"datasets/train\")\n",
    "    parser.add_argument(\"--val_data_path\", type=str, default=\"datasets/val\")\n",
    "    parser.add_argument(\"--LOAD_SIZE\", type=int, default=256)\n",
    "    parser.add_argument(\"--FINE_SIZE\", type=int, default=224)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=3)\n",
    "    parser.add_argument(\"--hidden\", nargs=\"*\", type=int, default=[])\n",
    "    parser.add_argument(\"--hidden_sz\", type=int, default=768)\n",
    "    parser.add_argument(\"--img_embed_pool_type\", type=str, default=\"avg\", choices=[\"max\", \"avg\"])\n",
    "    parser.add_argument(\"--img_hidden_sz\", type=int, default=512)\n",
    "    parser.add_argument(\"--include_bn\", type=int, default=True)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--lr_factor\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--lr_patience\", type=int, default=10)\n",
    "    parser.add_argument(\"--max_epochs\", type=int, default=500)\n",
    "    parser.add_argument(\"--n_workers\", type=int, default=12)\n",
    "    parser.add_argument(\"--name\", type=str, default=\"ReleasedVersion\")\n",
    "    parser.add_argument(\"--num_image_embeds\", type=int, default=1)\n",
    "    parser.add_argument(\"--patience\", type=int, default=20)\n",
    "    parser.add_argument(\"--savedir\", type=str, default=\"./savepath/ETMC/nyud2/\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1)\n",
    "    parser.add_argument(\"--n_classes\", type=int, default=10)\n",
    "    parser.add_argument(\"--annealing_epoch\", type=int, default=10)\n",
    "\n",
    "\n",
    "def get_optimizer(model, args):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, args):\n",
    "    return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"max\", patience=args.lr_patience, verbose=True, factor=args.lr_factor\n",
    "    )\n",
    "\n",
    "\n",
    "def model_forward(i_epoch, model, args, ce_loss, batch):\n",
    "    rgb, depth, tgt = batch['video_reshaped'], batch['video_reshaped'], batch['label_map']\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        rgb, depth, tgt = rgb.cuda(), depth.cuda(), tgt.cuda()\n",
    "    depth_alpha, rgb_alpha, pseudo_alpha, depth_rgb_alpha = model(rgb, depth)\n",
    "\n",
    "    loss = ce_loss(tgt, depth_alpha, args.n_classes, i_epoch, args.annealing_epoch) + \\\n",
    "           ce_loss(tgt, rgb_alpha, args.n_classes, i_epoch, args.annealing_epoch) + \\\n",
    "           ce_loss(tgt, pseudo_alpha, args.n_classes, i_epoch, args.annealing_epoch) + \\\n",
    "           ce_loss(tgt, depth_rgb_alpha, args.n_classes, i_epoch, args.annealing_epoch)\n",
    "    return loss, depth_alpha, rgb_alpha, depth_rgb_alpha, tgt\n",
    "\n",
    "\n",
    "def model_eval(i_epoch, data, model, args, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, depth_preds, rgb_preds, depthrgb_preds, tgts = [], [], [], [], []\n",
    "        for batch in data:\n",
    "            loss, depth_alpha, rgb_alpha, depth_rgb_alpha, tgt = model_forward(i_epoch, model, args, criterion, batch)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            depth_pred = depth_alpha.argmax(dim=1).cpu().detach().numpy()\n",
    "            rgb_pred = rgb_alpha.argmax(dim=1).cpu().detach().numpy()\n",
    "            depth_rgb_pred = depth_rgb_alpha.argmax(dim=1).cpu().detach().numpy()\n",
    "\n",
    "            depth_preds.append(depth_pred)\n",
    "            rgb_preds.append(rgb_pred)\n",
    "            depthrgb_preds.append(depth_rgb_pred)\n",
    "            tgt = tgt.cpu().detach().numpy()\n",
    "            tgts.append(tgt)\n",
    "\n",
    "    metrics = {\"loss\": np.mean(losses)}\n",
    "\n",
    "    tgts = [l for sl in tgts for l in sl]\n",
    "    depth_preds = [l for sl in depth_preds for l in sl]\n",
    "    rgb_preds = [l for sl in rgb_preds for l in sl]\n",
    "    depthrgb_preds = [l for sl in depthrgb_preds for l in sl]\n",
    "    metrics[\"depth_acc\"] = accuracy_score(tgts, depth_preds)\n",
    "    metrics[\"rgb_acc\"] = accuracy_score(tgts, rgb_preds)\n",
    "    metrics[\"depthrgb_acc\"] = accuracy_score(tgts, depthrgb_preds)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debonair\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'datasets/train'\n",
    "train_loader = DataLoader(\n",
    "        FakeAVCelebDataset(data_dir=train_data_path),\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.dfdt_dataset.FakeAVCelebDataset at 0x20173917f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = FakeAVCelebDataset(data_dir=train_data_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ds.samples:\n",
    "    print(item)\n",
    "    break\n",
    "# ds.samples.enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Module for loading the fakeavceleb dataset from tfrecord format'''\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "import tensorflow as tf\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.datasets.folder import make_dataset\n",
    "from torchdata.datapipes.iter import FileLister, FileOpener\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "FEATURE_DESCRIPTION = {\n",
    "    'video_path': tf.io.FixedLenFeature([], tf.string), \n",
    "    'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "    'clip/label/index': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'clip/label/text': tf.io.FixedLenFeature([], tf.string), \n",
    "    'WAVEFORM/feature/floats': tf.io.FixedLenFeature([], tf.string)\n",
    "}\n",
    "\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "@tf.function\n",
    "def _parse_function(example_proto):\n",
    "\n",
    "    #Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "    example = tf.io.parse_single_example(example_proto, FEATURE_DESCRIPTION)\n",
    "    \n",
    "    video_path = example['video_path']\n",
    "    video = tf.io.decode_raw(example['image/encoded'], tf.int8)\n",
    "    video_reshaped = video.reshape(10, 256, 256, 3)\n",
    "    \n",
    "    spectrogram = tf.io.decode_raw(example['WAVEFORM/feature/floats'], tf.float32)\n",
    "    spectrogram = spectrogram\n",
    "    \n",
    "    label = example[\"clip/label/text\"]\n",
    "    label_map = example[\"clip/label/index\"]\n",
    "    \n",
    "    return video_path, video_reshaped, spectrogram, label, label_map\n",
    "\n",
    "class FakeAVCelebDataset:\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.samples = self.load_features_from_tfrec()\n",
    "\n",
    "    def load_features_from_tfrec(self):\n",
    "        '''Loads raw features from a tfrecord file and returns them as raw inputs'''\n",
    "        ds = tf.io.matching_files(self.data_dir)\n",
    "        files = tf.random.shuffle(ds)\n",
    "        shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "        dataset = shards.interleave(tf.data.TFRecordDataset)\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.map(_parse_function, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        self.samples = self.load_features_from_tfrec(self.data_dir)\n",
    "        cnt = self.samples.reduce(np.int64(0), lambda x, _: x + 1)\n",
    "        cnt = cnt.numpy()\n",
    "        return cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(10, 256, 256, 3), dtype=tf.int8, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = FakeAVCelebDataset(data_dir='./datasets/train/fakeav*')\n",
    "ds = ds.load_features_from_tfrec()\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3) 0\n"
     ]
    }
   ],
   "source": [
    "for element in ds:\n",
    "    video_path, video_reshaped, spectrogram, label, label_map = element\n",
    "    video_frame = video_reshaped[0].numpy()\n",
    "    print(video_frame.shape, label_map.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    set_seed(args.seed)\n",
    "    args.savedir = os.path.join(args.savedir, args.name)\n",
    "    os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    model = ETMC(args)\n",
    "    optimizer = get_optimizer(model, args)\n",
    "    scheduler = get_scheduler(optimizer, args)\n",
    "    logger = create_logger(\"%s/logfile.log\" % args.savedir, args)\n",
    "\n",
    "    torch.save(args, os.path.join(args.savedir, \"args.pt\"))\n",
    "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
    "\n",
    "    if os.path.exists(os.path.join(args.savedir, \"checkpoint.pt\")):\n",
    "        checkpoint = torch.load(os.path.join(args.savedir, \"checkpoint.pt\"))\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        n_no_improve = checkpoint[\"n_no_improve\"]\n",
    "        best_metric = checkpoint[\"best_metric\"]\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "\n",
    "    for i_epoch in range(start_epoch, args.max_epochs):\n",
    "        train_losses = []\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch in ds:\n",
    "            video_path, video_reshaped, spectrogram, label, label_map = element\n",
    "            video_frame = video_reshaped[0].numpy()\n",
    "            label_map = label_map.numpy()\n",
    "            loss, depth_out, rgb_out, depthrgb, tgt = model_forward(i_epoch, model, args, ce_loss, batch)\n",
    "\n",
    "        for batch in tqdm(train_loader, total=len(train_loader)):\n",
    "            loss, depth_out, rgb_out, depthrgb, tgt = model_forward(i_epoch, model, args, ce_loss, batch)\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                 loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            global_step += 1\n",
    "            if global_step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        metrics = model_eval(\n",
    "            np.inf, test_loader, model, args, ce_loss\n",
    "        )\n",
    "        logger.info(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
    "        log_metrics(\"val\", metrics, logger)\n",
    "        logger.info(\n",
    "            \"{}: Loss: {:.5f} | depth_acc: {:.5f}, rgb_acc: {:.5f}, depth rgb acc: {:.5f}\".format(\n",
    "                \"val\", metrics[\"loss\"], metrics[\"depth_acc\"], metrics[\"rgb_acc\"], metrics[\"depthrgb_acc\"]\n",
    "            )\n",
    "        )\n",
    "        tuning_metric = metrics[\"depthrgb_acc\"]\n",
    "\n",
    "        scheduler.step(tuning_metric)\n",
    "        is_improvement = tuning_metric > best_metric\n",
    "        if is_improvement:\n",
    "            best_metric = tuning_metric\n",
    "            n_no_improve = 0\n",
    "        else:\n",
    "            n_no_improve += 1\n",
    "\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": i_epoch + 1,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"n_no_improve\": n_no_improve,\n",
    "                \"best_metric\": best_metric,\n",
    "            },\n",
    "            is_improvement,\n",
    "            args.savedir,\n",
    "        )\n",
    "\n",
    "        if n_no_improve >= args.patience:\n",
    "            logger.info(\"No improvement. Breaking out of loop.\")\n",
    "            break\n",
    "\n",
    "    load_checkpoint(model, os.path.join(args.savedir, \"model_best.pt\"))\n",
    "    model.eval()\n",
    "    test_metrics = model_eval(\n",
    "        np.inf, test_loader, model, args, ce_loss\n",
    "    )\n",
    "    logger.info(\n",
    "        \"{}: Loss: {:.5f} | depth_acc: {:.5f}, rgb_acc: {:.5f}, depth rgb acc: {:.5f}\".format(\n",
    "            \"Test\", test_metrics[\"loss\"], test_metrics[\"depth_acc\"], test_metrics[\"rgb_acc\"],\n",
    "            test_metrics[\"depthrgb_acc\"]\n",
    "        )\n",
    "    )\n",
    "    log_metrics(f\"Test\", test_metrics, logger)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
